{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMedSoQAMcaRwXuIGrU/FM+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GizawAAiT/Deep-Learning/blob/main/Lab_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZftUiER4KkFR"
      },
      "outputs": [],
      "source": [
        "# Importing torch\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_Layer:\n",
        "  def __init__(self, features, neurons):\n",
        "    self.features= features\n",
        "    self.neurons = neurons\n",
        "    self.weights = torch.rand(features, neurons)\n",
        "    self.bias = torch.rand(neurons)\n",
        "\n",
        "  # Do forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = inputs@self.weights + self.bias\n",
        "\n",
        "  # relu activation\n",
        "  def relu_activation(self, x):\n",
        "    self.relu = torch.max(x, torch.tensor(0))\n",
        "\n",
        "  # sigmoid activation\n",
        "  def sigmoid_activation(self, x):\n",
        "    self.sigmoid = 1/(1 + torch.exp(-x))\n",
        "\n",
        "  # softmax activation\n",
        "  def softmax_activation(self, x):\n",
        "    self.softmax = torch.exp(x)/torch.sum(torch.exp(x), axis = 1, keepdim = True)\n",
        "\n",
        "  # categorical loss cross entropy C.L.C.E\n",
        "  def categoricalCrossentropy(self, y_true, y_predicted):\n",
        "    self.loss = torch.mean(-torch.sum(y_true*torch.log(y_predicted), axis = 1))\n",
        "    return self.loss"
      ],
      "metadata": {
        "id": "U4rHEvq1NHlq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create sample data\n",
        "x = torch.rand(32, 4)\n",
        "# x"
      ],
      "metadata": {
        "id": "5RGB3rN1P518"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create sample true value\n",
        "y_true = torch.zeros(32, 18)\n",
        "indices = torch.arange(32) % 3\n",
        "y_true[torch.arange(32), indices] = 1\n",
        "\n",
        "# print(f'y_true:\\n{y_true}')"
      ],
      "metadata": {
        "id": "tUYgEdVbXb2K"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating layers:\n",
        "hidden_layer1 = Neural_Layer(4, 18)\n",
        "hidden_layer2 =Neural_Layer(18, 18)\n",
        "hidden_layer3 = Neural_Layer(18, 18)\n",
        "output_layer = Neural_Layer(3, 3)\n",
        "\n",
        "hidden_layer1.forward(x)\n",
        "hidden_layer1.relu_activation(hidden_layer1.output)\n",
        "\n",
        "hidden_layer2.relu_activation(hidden_layer1.relu)\n",
        "\n",
        "hidden_layer3.softmax_activation(hidden_layer2.relu)\n",
        "output_layer.sigmoid_activation(hidden_layer3.softmax)\n",
        "loss = output_layer.categoricalCrossentropy(y_true, output_layer.sigmoid)\n",
        "print(f'loss = {loss*100} \\nAccuracy = {100*(1-loss)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjRiEGrGP1Th",
        "outputId": "88dd57bb-30af-4c51-a749-2098a15807d0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 65.91790008544922 \n",
            "Accuracy = 34.08210372924805\n"
          ]
        }
      ]
    }
  ]
}